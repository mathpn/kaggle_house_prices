{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(corrplot)\n",
    "library(ggplot2)\n",
    "library(tidyr)\n",
    "library(dplyr)\n",
    "library(purrr)\n",
    "library(GGally)\n",
    "library(MASS)\n",
    "library(caret)\n",
    "library(glmnet)\n",
    "library(Rcpp)\n",
    "library(randomForest)\n",
    "library(doParallel)\n",
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll analyze the Kaggle House Prices dataset and predict the prices of houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data <- read.csv(\"data/train.csv\")\n",
    "test_data <- read.csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking at look at which variables are available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "str(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a mix of numerical and categorical variables with missing values. Let's take a look at the sale price variable, which is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qplot(train_data$SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the distribution of SalePrice has a positive skew and is not exactly normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 20, repr.plot.height = 20)\n",
    "nums <- unlist(lapply(train_data, is.numeric), use.names = FALSE)\n",
    "\n",
    "train_data %>%\n",
    "select_if(is.numeric) %>%\n",
    "gather(cols, value) %>%\n",
    "ggplot(aes(value)) + geom_histogram() + facet_wrap(~cols, scales = 'free')\n",
    "\n",
    "options(repr.plot.width = 12, repr.plot.height = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, many variables are not normally distributed. Also, many of them seem to be count variables with discrete values. Some have very strong skew and kurtosis, while others have zero-inflated distributions. These facts are important to understand before fitting a model.\n",
    "\n",
    "Let's also have a look at correlation matrix plots to see if we can spot some obvious or interesting correlations. We'll use Spearman correlation since most variables are not normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data %>%\n",
    "select_if(is.numeric) %>%\n",
    "cor(use = \"complete.obs\", method = \"spearman\") -> correlations\n",
    "\n",
    "corrplot(correlations, method = \"circle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables seem to provide little extra information over others, for example, YearBuilt and GarageYrBlt. This means there is quite a lot of multicolinearity in the data. This is very relevant if we want to fit a linear model.\n",
    "\n",
    "Let's finally take a look at the variables with the highest correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "correlations[, 38] %>% sort(decreasing = TRUE) %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall quality, ground living area, year built, garage capacity and the number of full bathrooms are the variables most correlated with the target variable.\n",
    "\n",
    "Let's take a look at the number of levels of each categorical variable. Variables with only two levels are transformed to numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data <- as.data.frame(unclass(train_data), stringsAsFactors = TRUE)\n",
    "\n",
    "train_data$Street <- as.numeric(train_data$Street)\n",
    "train_data$Alley <- as.numeric(train_data$Alley)\n",
    "train_data$Utilities <- as.numeric(train_data$Utilities)\n",
    "train_data$CentralAir <- as.numeric(train_data$CentralAir)\n",
    "\n",
    "\n",
    "# test data\n",
    "test_data <- as.data.frame(unclass(test_data), stringsAsFactors = TRUE)\n",
    "\n",
    "test_data$Street <- as.numeric(test_data$Street)\n",
    "test_data$Alley <- as.numeric(test_data$Alley)\n",
    "test_data$Utilities <- as.numeric(test_data$Utilities)\n",
    "test_data$CentralAir <- as.numeric(test_data$CentralAir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lapply(train_data, is.na) %>%\n",
    "sapply(sum) %>%\n",
    "sapply(function(x) x / 1460) %>%\n",
    "sort(decreasing = TRUE) -> na_proportion\n",
    "\n",
    "print(na_proportion[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll simply remove the variables with more than 20% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data_cleaned <- subset(\n",
    "    train_data,\n",
    "    select = -c(PoolQC, MiscFeature, Alley, Fence, FireplaceQu)\n",
    ")\n",
    "\n",
    "test_data_cleaned <- subset(\n",
    "    test_data,\n",
    "    select = -c(PoolQC, MiscFeature, Alley, Fence, FireplaceQu)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the variables with the highest correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "correlations[, 38] %>% sort(decreasing = TRUE) %>% names() -> cols\n",
    "\n",
    "train_data %>%\n",
    "dplyr::select(cols[0:10]) %>%\n",
    "ggpairs(aes(alpha = 0.75)) + theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first column, we can see that in fact those variables seem to be positively correlated with the target variable. However, only overall quality seems to be reasonably normally distributed. Let's take a look at their distributions and transform them to get more normal-like distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 6, repr.plot.height=6)\n",
    "\n",
    "qqnorm(train_data_cleaned$SalePrice)\n",
    "qqline(train_data_cleaned$SalePrice)\n",
    "\n",
    "train_data_cleaned$SalePriceLog <- log(train_data_cleaned$SalePrice)\n",
    "qqnorm(train_data_cleaned$SalePriceLog)\n",
    "qqline(train_data_cleaned$SalePriceLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(test_data_cleaned$GrLivArea)\n",
    "qqline(test_data_cleaned$GrLivArea)\n",
    "\n",
    "train_data_cleaned$GrLivAreaLog <- log(train_data_cleaned$GrLivArea)\n",
    "test_data_cleaned$GrLivAreaLog <- log(test_data_cleaned$GrLivArea)\n",
    "qqnorm(test_data_cleaned$GrLivAreaLog)\n",
    "qqline(test_data_cleaned$GrLivAreaLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(test_data_cleaned$YearBuilt)\n",
    "qqline(test_data_cleaned$YearBuilt)\n",
    "\n",
    "train_data_cleaned$YearBuiltLogInv <- log(1 / train_data_cleaned$YearBuilt)\n",
    "test_data_cleaned$YearBuiltLogInv <- log(1 / test_data_cleaned$YearBuilt)\n",
    "qqnorm(train_data_cleaned$YearBuiltLogInv)\n",
    "qqline(train_data_cleaned$YearBuiltLogInv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is so skewed the transformation is not enough to make it normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(train_data_cleaned$GarageArea)\n",
    "qqline(train_data_cleaned$GarageArea)\n",
    "\n",
    "train_data_cleaned$GarageAreaLog <- log(train_data_cleaned$GarageArea + 1)\n",
    "test_data_cleaned$GarageAreaLog <- log(test_data_cleaned$GarageArea + 1)\n",
    "qqnorm(train_data_cleaned$GarageAreaLog)\n",
    "qqline(train_data_cleaned$GarageAreaLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this one has a lot of zeros, we'll create a new variable indicating if a garage is present or not. We'll also turn the zero values into NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data_cleaned$GaragePresent <- as.numeric(\n",
    "    train_data_cleaned$GarageArea > 0\n",
    ")\n",
    "train_data_cleaned$GarageAreaLog <- ifelse(\n",
    "    train_data_cleaned$GaragePresent,\n",
    "    train_data_cleaned$GarageAreaLog,\n",
    "    NA\n",
    ")\n",
    "\n",
    "summary(train_data_cleaned$GarageAreaLog)\n",
    "summary(train_data_cleaned$GaragePresent)\n",
    "\n",
    "test_data_cleaned$GaragePresent <- as.numeric(test_data_cleaned$GarageArea > 0)\n",
    "test_data_cleaned$GarageAreaLog <- ifelse(\n",
    "    test_data_cleaned$GaragePresent,\n",
    "    train_data_cleaned$GarageAreaLog,\n",
    "    NA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(train_data_cleaned$GarageYrBlt)\n",
    "qqline(train_data_cleaned$GarageYrBlt)\n",
    "\n",
    "train_data_cleaned$GarageYrBltLogInv <- log(1 / train_data_cleaned$GarageYrBlt)\n",
    "test_data_cleaned$GarageYrBltLogInv <- log(1 / test_data_cleaned$GarageYrBlt)\n",
    "qqnorm(train_data_cleaned$GarageYrBltLogInv)\n",
    "qqline(train_data_cleaned$GarageYrBltLogInv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(train_data_cleaned$TotalBsmtSF)\n",
    "qqline(train_data_cleaned$TotalBsmtSF)\n",
    "\n",
    "train_data_cleaned$TotalBsmtSFLog <- log(1 + train_data_cleaned$TotalBsmtSF)\n",
    "test_data_cleaned$TotalBsmtSFLog <- log(1 + test_data_cleaned$TotalBsmtSF)\n",
    "qqnorm(train_data_cleaned$TotalBsmtSFLog)\n",
    "qqline(train_data_cleaned$TotalBsmtSFLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it's useful to create a new variable indicating if a basement is present or not and turn zeros into NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data_cleaned$BasementPresent <- as.numeric(\n",
    "    train_data_cleaned$TotalBsmtSF > 0\n",
    ")\n",
    "train_data_cleaned$TotalBsmtSFLog <- ifelse(\n",
    "    train_data_cleaned$BasementPresent,\n",
    "    train_data_cleaned$TotalBsmtSFLog,\n",
    "    NA\n",
    ")\n",
    "summary(train_data_cleaned$TotalBsmtSFLog)\n",
    "summary(train_data_cleaned$BasementPresent)\n",
    "\n",
    "test_data_cleaned$BasementPresent <- as.numeric(\n",
    "    test_data_cleaned$TotalBsmtSF > 0\n",
    ")\n",
    "test_data_cleaned$TotalBsmtSFLog <- ifelse(\n",
    "    test_data_cleaned$BasementPresent,\n",
    "    train_data_cleaned$TotalBsmtSFLog,\n",
    "    NA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qplot(\n",
    "    train_data_cleaned$YearRemodAdd,\n",
    "    train_data_cleaned$SalePrice,\n",
    "    geom = \"point\"\n",
    ")\n",
    "qqnorm(train_data_cleaned$YearRemodAdd)\n",
    "qqline(train_data_cleaned$YearRemodAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again too skewed and there's also a suspicious excess of 1950s in this variable. For this reason, We'll binarize this variable around 1985 (by visual inspection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data_cleaned$YearRemodAddBinary <- as.numeric(\n",
    "    train_data_cleaned$YearRemodAdd >= 1985\n",
    ")\n",
    "summary(train_data_cleaned$YearRemodAddBinary)\n",
    "\n",
    "test_data_cleaned$YearRemodAddBinary <- as.numeric(\n",
    "    test_data_cleaned$YearRemodAdd >= 1985\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "qqnorm(train_data_cleaned$X1stFlrSF)\n",
    "qqline(train_data_cleaned$X1stFlrSF)\n",
    "\n",
    "train_data_cleaned$X1stFlrSFLog <- log(train_data_cleaned$X1stFlrSF)\n",
    "test_data_cleaned$X1stFlrSFLog <- log(test_data_cleaned$X1stFlrSF)\n",
    "qqnorm(train_data_cleaned$X1stFlrSFLog)\n",
    "qqline(train_data_cleaned$X1stFlrSFLog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now we have a data table with transformed variables and some new ones for those that are most correlated with the target variable. Let's visualize again the correlation patterns between the target variables and the transformed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 12, repr.plot.height = 12)\n",
    "\n",
    "transformed_cols = c(\n",
    "    \"SalePriceLog\", \"OverallQual\", \"GrLivAreaLog\",\n",
    "    \"YearBuiltLogInv\", \"GarageCars\", \"FullBath\",\n",
    "    \"GarageAreaLog\", \"GarageYrBltLogInv\", \"TotalBsmtSFLog\",\n",
    "    \"YearRemodAddBinary\", \"X1stFlrSFLog\")\n",
    "\n",
    "train_data_cleaned %>%\n",
    "dplyr::select(transformed_cols) %>%\n",
    "ggpairs(aes(alpha = 0.75)) + theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column looks better now with the transformed variables. Let's split the data into train and validation sets and then fit a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "split_index <- caret::createDataPartition(\n",
    "    train_data_cleaned$SalePriceLog, p = 0.8, list = FALSE\n",
    ")\n",
    "train_data_cleaned_train <- train_data_cleaned[split_index, ]\n",
    "train_data_cleaned_val <- train_data_cleaned[-split_index, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "formula <- paste(\n",
    "    \"SalePriceLog ~ \",\n",
    "    paste(transformed_cols[-1], collapse = \" + \"),\n",
    "    sep = \"\"\n",
    ")\n",
    "\n",
    "linear_model <- lm(formula, data = train_data_cleaned_train)\n",
    "\n",
    "summary(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the root mean squared error, which is the metric used in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predictions <- linear_model %>% predict(train_data_cleaned_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        train_data_cleaned_val$SalePriceLog,\n",
    "        na.rm = TRUE\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        train_data_cleaned_val$SalePriceLog,\n",
    "        na.rm = TRUE\n",
    "        )\n",
    ") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation plots we can see there's quite some multicolinearity happening here. Still, the model doesn't seem all that bad with R^2 = 0.82.\n",
    "\n",
    "We should take this with a grain of salt since there are far too many categorical variables and levels and too much multicolinearity. It's a good idea to either use a form of variable selection such as lasso or another type of model that can implicitly handle the excess of variables.\n",
    "\n",
    "We'll start with lasso and ridge penalized regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "near_zero_var <- caret::nearZeroVar(train_data_cleaned_train)\n",
    "train_data_variance <- train_data_cleaned_train[, -near_zero_var]\n",
    "\n",
    "lambdas <- 10 ^ seq(-5, 5, length = 100)\n",
    "\n",
    "ridge <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance[, -which(\n",
    "        names(train_data_variance) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneGrid = expand.grid(alpha = 0, lambda = lambdas),\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "lasso <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance[, -which(\n",
    "        names(train_data_variance) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneGrid = expand.grid(alpha = 1, lambda = lambdas),\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "elastic <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance[, -which(\n",
    "        names(train_data_variance) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneLength = 10,\n",
    "    na.action = na.omit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check which were the optimal lambdas and the RMSE for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ridge reggression:\")\n",
    "print(ridge$bestTune)\n",
    "predictions <- ridge %>% predict(train_data_cleaned_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()\n",
    "\n",
    "\n",
    "print(\"Lasso reggression:\")\n",
    "print(lasso$bestTune)\n",
    "predictions <- lasso %>% predict(train_data_cleaned_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()\n",
    "\n",
    "print(\"Elastic net reggression:\")\n",
    "print(lasso$bestTune)\n",
    "predictions <- elastic %>% predict(train_data_cleaned_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the best model was the simple linear model with only the top-10 most-correlated-with-outcome variables. Before continuing, let's first go back to our data and do two things:\n",
    "\n",
    "    1) Standardize/normalize the data\n",
    "    2) Remove possible outliers\n",
    "\n",
    "Let's first check how many observations are over 4 standard deviations from the mean for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_data_cleaned %>%\n",
    "select_if(is.numeric) %>%\n",
    "na.omit() %>%\n",
    "lapply(scale) %>%\n",
    "lapply(function(x) abs(x) > 3) %>%\n",
    "sapply(sum) %>%\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll clip the values to be between -3 and 3 standard deviations from the mean and then normalize all numerical variables to the range (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "clip <- function(vec, max=3, min=-3) pmax(min, pmin(vec, max))\n",
    "\n",
    "train_data_cleaned %>%\n",
    "select_if(is.numeric) %>%\n",
    "lapply(scale) %>%\n",
    "sapply(clip) %>%\n",
    "data.frame() ->\n",
    "train_data_scaled\n",
    "\n",
    "train_data_scaled %>%\n",
    "caret::preProcess(method = \"range\") ->\n",
    "scaler\n",
    "\n",
    "train_data_norm <- scaler %>% predict(train_data_scaled)\n",
    "\n",
    "train_data_cleaned %>%\n",
    "select_if(purrr::negate(is.numeric)) %>%\n",
    "names() -> categorical_cols\n",
    "\n",
    "train_data_norm <- cbind(\n",
    "    train_data_norm, train_data_cleaned[, categorical_cols]\n",
    ")\n",
    "\n",
    "train_data_norm <- train_data_norm[, -which(\n",
    "    names(train_data_norm) %in% c(\"Id\", \"SalePrice\")\n",
    ")]\n",
    "train_data_norm$SalePriceLog <- train_data_cleaned$SalePriceLog\n",
    "\n",
    "train_data_norm_train <- train_data_norm[split_index, ]\n",
    "train_data_norm_val <- train_data_norm[-split_index, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit our linear models one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "formula <- paste(\n",
    "    \"SalePriceLog ~ \",\n",
    "    paste(transformed_cols[-1], collapse = \" + \"),\n",
    "    sep = \"\"\n",
    ")\n",
    "\n",
    "linear_model <- lm(formula, data = train_data_norm_train)\n",
    "\n",
    "summary(linear_model)\n",
    "\n",
    "(predict(linear_model, train_data_norm_val) -\n",
    "    train_data_norm_val$SalePriceLog) %>%\n",
    "mean(na.rm = TRUE) %>%\n",
    "sqrt() -> lm_rmse\n",
    "\n",
    "predictions <- linear_model %>% predict(train_data_norm_val)\n",
    "\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        train_data_norm_val$SalePriceLog,\n",
    "        na.rm = TRUE\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        train_data_norm_val$SalePriceLog,\n",
    "        na.rm = TRUE\n",
    "        )\n",
    ") %>% print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "near_zero_var <- caret::nearZeroVar(train_data_norm_train)\n",
    "train_data_variance <- train_data_norm_train[, -near_zero_var]\n",
    "\n",
    "lambdas <- 10 ^ seq(-5, 5, length = 100)\n",
    "ridge <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance,\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneGrid = expand.grid(alpha = 0, lambda = lambdas),\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "lasso <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance,\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneGrid = expand.grid(alpha = 1, lambda = lambdas),\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "elastic <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance,\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneLength = 10,\n",
    "    na.action = na.omit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ridge reggression:\")\n",
    "print(ridge$bestTune)\n",
    "predictions <- ridge %>% predict(train_data_norm_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_norm_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_norm_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()\n",
    "\n",
    "\n",
    "print(\"Lasso reggression:\")\n",
    "print(lasso$bestTune)\n",
    "predictions <- lasso %>% predict(train_data_norm_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_norm_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_norm_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()\n",
    "\n",
    "print(\"Elastic net reggression:\")\n",
    "print(lasso$bestTune)\n",
    "predictions <- elastic %>% predict(train_data_norm_val)\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_norm_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_norm_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no clear improvement and RMSE for the penalized models even went down, so let's switch back to the un-normalized data. I think we should now switch to another more powerful model. Let's juice up and the popular Random Forest and XGBoost models.\n",
    "\n",
    "We'll use randomForest's default parameters and doParallel to parallelize the computation. Please adjust the number of threads so your CPU doesn't explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "near_zero_var <- caret::nearZeroVar(train_data_cleaned_train)\n",
    "train_data_variance <- train_data_cleaned_train[, -near_zero_var]\n",
    "\n",
    "lambdas <- 10 ^ seq(-5, 5, length = 100)\n",
    "\n",
    "ridge <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance[, -which(\n",
    "        names(train_data_variance) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneGrid = expand.grid(alpha = 0, lambda = lambdas),\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "lasso <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance[, -which(\n",
    "        names(train_data_variance) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneGrid = expand.grid(alpha = 1, lambda = lambdas),\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "elastic <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_variance[, -which(\n",
    "        names(train_data_variance) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"glmnet\",\n",
    "    trControl = trainControl(\"cv\", number = 5),\n",
    "    tuneLength = 10,\n",
    "    na.action = na.omit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cl <- makePSOCKcluster(16)\n",
    "registerDoParallel(cl)\n",
    "\n",
    "rf_model <- caret::train(\n",
    "    SalePriceLog ~ .,\n",
    "    data = train_data_cleaned_train[, -which(\n",
    "        names(train_data_cleaned_train) == \"SalePrice\"\n",
    "        )],\n",
    "    method = \"rf\",\n",
    "    na.action = na.omit\n",
    ")\n",
    "\n",
    "stopCluster(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predictions <- predict(rf_model, train_data_cleaned_val)\n",
    "\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little hack is required to finish background parallel tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#from https://stackoverflow.com/questions/64519640/error-in-summary-connectionconnection-invalid-connection\n",
    "unregister_dopar <- function() {\n",
    "  env <- foreach:::.foreachGlobals\n",
    "  rm(list=ls(name=env), pos=env)\n",
    "}\n",
    "\n",
    "unregister_dopar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! There might be some overfitting, but I think it's a good starting point. It's time to go XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_control <- trainControl(method = \"cv\", number = 3)\n",
    "\n",
    "xgb_grid <- expand.grid(nrounds = 500,\n",
    "                        max_depth = seq(2, 9),\n",
    "                        eta = c(0.1, 0.3, 1),\n",
    "                        gamma = c(0.0, 0.01, 0.1, 1),\n",
    "                        min_child_weight = c(0.5, 1, 2, 4),\n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 0.5\n",
    ")\n",
    "\n",
    "xgb_tune <- train(SalePriceLog ~.,\n",
    "                 data = train_data_cleaned_train[, -which(\n",
    "                     names(train_data_cleaned_train) == \"SalePrice\"\n",
    "                     )],\n",
    "                 method = \"xgbTree\",\n",
    "                 metric = \"RMSE\",\n",
    "                 trControl = train_control,\n",
    "                 tuneGrid = xgb_grid,\n",
    "                 na.action = na.pass\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predictions <- predict(xgb_tune, train_data_cleaned_val)\n",
    "\n",
    "data.frame(\n",
    "    RMSE = RMSE(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        ),\n",
    "    R2 = R2(\n",
    "        predictions,\n",
    "        tidyr::drop_na(train_data_cleaned_val)$SalePriceLog\n",
    "        )\n",
    ") %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite surprisingly, ridge regression is still the best model! We'll then use it to produce our test predictions. We can also contruct an ensemble with the models we've trained so far. Let's do the same scaling and outlier removal as we did for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(dim(test_data_cleaned))\n",
    "\n",
    "test_data_cleaned %>%\n",
    "select_if(is.numeric) %>%\n",
    "lapply(scale) %>%\n",
    "sapply(clip) %>%\n",
    "data.frame() ->\n",
    "test_data_scaled\n",
    "\n",
    "test_data_scaled %>%\n",
    "caret::preProcess(method = \"range\") ->\n",
    "scaler\n",
    "\n",
    "test_data_norm <- scaler %>% predict(test_data_scaled)\n",
    "\n",
    "test_data_cleaned %>%\n",
    "select_if(purrr::negate(is.numeric)) %>%\n",
    "names() -> categorical_cols\n",
    "\n",
    "test_data_norm <- cbind(\n",
    "    test_data_norm, test_data_cleaned[, categorical_cols]\n",
    ")\n",
    "\n",
    "test_data_norm <- test_data_norm[, -which(\n",
    "    names(test_data_norm) %in% c(\"Id\", \"SalePrice\")\n",
    ")]\n",
    "test_data_norm$SalePriceLog <- test_data_cleaned$SalePriceLog\n",
    "\n",
    "print(dim(test_data_cleaned))\n",
    "print(dim(test_data_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predict(xgb_tune, test_data_cleaned) %>% exp() -> test_predictions\n",
    "length(test_predictions)\n",
    "\n",
    "submission <- data.frame(\n",
    "    Id = test_data$Id,\n",
    "    SalePrice = test_predictions\n",
    ")\n",
    "\n",
    "write.csv(submission, \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
